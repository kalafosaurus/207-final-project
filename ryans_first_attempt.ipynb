{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# tf and keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from PIL import ImageFile\n",
    "\n",
    "# sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(metadata_path='fungi-clef-2025/metadata/FungiTastic-FewShot/', image_path='fungi-clef-2025/images/FungiTastic-FewShot/'):\n",
    "    \"\"\"Load the metadata for each data split.\"\"\"\n",
    "    # Load the metadata for each split\n",
    "    train_metadata = pd.read_csv(os.path.join(metadata_path, 'FungiTastic-FewShot-Train.csv'))\n",
    "    val_metadata = pd.read_csv(os.path.join(metadata_path, 'FungiTastic-FewShot-Val.csv'))\n",
    "    test_metadata = pd.read_csv(os.path.join(metadata_path, 'FungiTastic-FewShot-Test.csv'))\n",
    "    \n",
    "    # Label each split\n",
    "    train_metadata[\"split\"] = \"train\"\n",
    "    val_metadata[\"split\"] = \"val\"\n",
    "    test_metadata[\"split\"] = \"test\"\n",
    "\n",
    "    # Join all of the data together\n",
    "    df_metadata = pd.concat([train_metadata, val_metadata, test_metadata])\n",
    "\n",
    "    # Add the full image location for each image\n",
    "    # Options for image size include 300p, 500p, 720p, fullsize \n",
    "    df_metadata[\"image_path\"] = df_metadata.apply(\n",
    "        lambda row: os.path.join(image_path, f\"{row['split']}/300p/{row['filename']}\"), axis=1\n",
    "    )\n",
    "\n",
    "    return df_metadata\n",
    "\n",
    "\n",
    "def filter_low_counts(df, min_samples):\n",
    "    \"\"\"Filter out examples of fungi with low value counts.\"\"\"\n",
    "    class_counts = df[\"class\"].value_counts()\n",
    "    frequent_classes = class_counts[class_counts >= min_samples].index\n",
    "    filtered_df = df[df[\"class\"].isin(frequent_classes)]\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def resize_with_aspect_ratio(image, target_size):\n",
    "    # Get original dimensions\n",
    "    width, height = image.size\n",
    "    \n",
    "    # Calculate scaling factor\n",
    "    if width > height:\n",
    "        new_width = target_size\n",
    "        new_height = int(target_size * height / width)\n",
    "    else:\n",
    "        new_height = target_size\n",
    "        new_width = int(target_size * width / height)\n",
    "    \n",
    "    # Resize the image\n",
    "    return image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "\n",
    "def add_padding(image, target_size):\n",
    "    # Calculate padding\n",
    "    width, height = image.size\n",
    "    delta_w = target_size - width\n",
    "    delta_h = target_size - height\n",
    "    padding = (delta_w // 2, delta_h // 2, delta_w - delta_w // 2, delta_h - delta_h // 2)\n",
    "    \n",
    "    # Add padding (black by default)\n",
    "    return ImageOps.expand(image, padding, fill=(0, 0, 0))  # Use fill=(255,255,255) for white padding\n",
    "\n",
    "\n",
    "def preprocess_image(image_path, target_size):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Resize while maintaining aspect ratio\n",
    "    resized_image = resize_with_aspect_ratio(image, target_size)\n",
    "    \n",
    "    # Add padding to make it square\n",
    "    padded_image = add_padding(resized_image, target_size)\n",
    "    \n",
    "    return padded_image\n",
    "\n",
    "\n",
    "def load_images_and_labels(df, image_size):\n",
    "    \"\"\"Load the images and labels based on the metadata frame passed in.\"\"\"\n",
    "    images = []\n",
    "    labels_class = []\n",
    "    labels_poison = []\n",
    "    labels_species = []\n",
    "    variables = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Load and save the image as an array\n",
    "        # img = load_img(row[\"image_path\"], target_size=image_size)\n",
    "        img = preprocess_image(row[\"image_path\"], image_size)\n",
    "        img_arr = img_to_array(img)\n",
    "        images.append(img_arr)\n",
    "\n",
    "        # Append the class to the list of labels\n",
    "        labels_class.append(row[\"class_idx\"])\n",
    "\n",
    "        labels_poison.append(row[\"poisonous\"])\n",
    "        labels_species.append(row[\"species_idx\"])\n",
    "        variables.append((row[\"latitude\"], row[\"longitude\"], row[\"elevation\"], row[\"countryCode\"], row[\"region\"], row[\"substrate\"], row[\"habitat\"], row[\"landcover\"]))\n",
    "\n",
    "    # Stack and convert into a numpy array\n",
    "    images = np.stack(images)\n",
    "\n",
    "    # Rescale all of the images so they're pixel value 0 - 1\n",
    "    images = images / 255.0\n",
    "\n",
    "    # Cast label list to np.array for easier manipulation\n",
    "    labels_class = np.array(labels_class)\n",
    "    labels_poison = np.array(labels_poison)\n",
    "    labels_species = np.array(labels_species)\n",
    "    variables = np.array(variables)\n",
    "\n",
    "    return images, labels_class, labels_poison, labels_species, variables\n",
    "\n",
    "# ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# def load_images_and_labels(df, image_size):\n",
    "#     \"\"\"Load the images and labels based on the metadata frame passed in.\"\"\"\n",
    "#     images = []\n",
    "#     labels = []\n",
    "\n",
    "#     for idx, row in df.iterrows():\n",
    "#         try:\n",
    "#             # Load and save the image as an array\n",
    "#             img = load_img(row[\"image_path\"], target_size=image_size)\n",
    "#             img_arr = img_to_array(img)\n",
    "#             images.append(img_arr)\n",
    "\n",
    "#             # Append the class to the list of labels\n",
    "#             labels.append(row[\"class\"])\n",
    "#         except (OSError, FileNotFoundError) as e:\n",
    "#             # Handle corrupted or missing image files\n",
    "#             print(f\"Skipping image {row['image_path']} due to error: {e}\")\n",
    "\n",
    "#     # Stack and convert into a numpy array\n",
    "#     images = np.stack(images)\n",
    "\n",
    "#     # Rescale all of the images so they're pixel value 0 - 1\n",
    "#     images = images / 255.0\n",
    "\n",
    "#     # Cast label list to np.array for easier manipulation\n",
    "#     labels = np.array(labels)\n",
    "\n",
    "#     return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Variables\n",
    "IMAGE_SIZE = (224, 224)  # The size images should be rescaled to. If None, defaults to original size\n",
    "MIN_SAMPLES = 5  # The minimum number of samples needed to be included in a prediction\n",
    "\n",
    "# Load the metadata\n",
    "md_df = load_metadata()\n",
    "    \n",
    "# Filter out all the fungi that don't have the min number of samples\n",
    "# This might have been dropping the full test set oops\n",
    "# md_df = filter_low_counts(md_df, MIN_SAMPLES)\n",
    "\n",
    "# Map the class to an ID\n",
    "le = LabelEncoder()\n",
    "le.fit(md_df[\"class\"])\n",
    "md_df[\"class_label\"] = md_df[\"class\"]\n",
    "md_df[\"class_idx\"] = le.transform(md_df[\"class\"])\n",
    "le.fit(md_df[\"species\"])\n",
    "md_df[\"species_label\"] = md_df[\"species\"]\n",
    "md_df[\"species_idx\"] = le.transform(md_df[\"species\"])\n",
    "\n",
    "# Load all of the images and labels from the metadata\n",
    "# This function currently resizes and rescales the images\n",
    "images, labels_class, labels_poison, labels_species, variables = load_images_and_labels(md_df, 224)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-split the images and their labels\n",
    "train_idx = md_df[\"split\"] == \"train\"\n",
    "val_idx = md_df[\"split\"] == \"val\"\n",
    "test_idx = md_df[\"split\"] == \"test\"\n",
    "\n",
    "train_images = images[train_idx]\n",
    "train_labels_class = labels_class[train_idx]\n",
    "train_labels_poison = labels_poison[train_idx]\n",
    "train_labels_species = labels_species[train_idx]\n",
    "train_variables = variables[train_idx]\n",
    "\n",
    "val_images = images[val_idx]\n",
    "val_labels_class = labels_class[val_idx]\n",
    "val_labels_poison = labels_poison[val_idx]\n",
    "val_labels_species = labels_species[val_idx]\n",
    "val_variables = variables[val_idx]\n",
    "\n",
    "test_images = images[test_idx]\n",
    "test_labels_class = labels_class[test_idx]\n",
    "test_labels_poison = labels_poison[test_idx]\n",
    "test_labels_species = labels_species[test_idx]\n",
    "test_variables = variables[test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train images: (7819, 224, 224, 3)\n",
      "Shape train classes: (7819,)\n",
      "Shape train poison: (7819,)\n",
      "Shape train species: (7819,)\n",
      "Shape train variables: (7819, 8)\n",
      "Shape val images: (2285, 224, 224, 3)\n",
      "Shape test images: (1911, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape train images: {train_images.shape}\")\n",
    "print(f\"Shape train classes: {train_labels_class.shape}\")\n",
    "print(f\"Shape train poison: {train_labels_poison.shape}\")\n",
    "print(f\"Shape train species: {train_labels_species.shape}\")\n",
    "print(f\"Shape train variables: {train_variables.shape}\")\n",
    "print(f\"Shape val images: {val_images.shape}\")\n",
    "print(f\"Shape test images: {test_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the training images\n",
    "indices = list(range(train_images.shape[0]))  # create a list of indices of the size of the dataset\n",
    "\n",
    "shuffled_indices = np.random.permutation(indices)  # shuffle the indices\n",
    "\n",
    "train_images_shuffled = train_images[shuffled_indices]  # shuffle the rows of the dataset\n",
    "train_labels_class_shuffled = train_labels_class[shuffled_indices]\n",
    "train_labels_poison_shuffled = train_labels_poison[shuffled_indices]\n",
    "train_labels_species_shuffled = train_labels_species[shuffled_indices]\n",
    "train_variables_shuffled = train_variables[shuffled_indices]\n",
    "\n",
    "\n",
    "# Shuffle the validation images\n",
    "indices = list(range(val_images.shape[0]))  # create a list of indices of the size of the dataset\n",
    "shuffled_indices = np.random.permutation(indices)  # shuffle the indices\n",
    "val_images_shuffled = val_images[shuffled_indices]  # shuffle the rows of the dataset\n",
    "val_labels_class_shuffled = val_labels_class[shuffled_indices]\n",
    "val_labels_poison_shuffled = val_labels_poison[shuffled_indices]\n",
    "val_labels_species_shuffled = val_labels_species[shuffled_indices]\n",
    "val_variables_shuffled = val_variables[shuffled_indices]\n",
    "\n",
    "\n",
    "# Shuffle the test images\n",
    "indices = list(range(test_images.shape[0]))  # create a list of indices of the size of the dataset\n",
    "shuffled_indices = np.random.permutation(indices)  # shuffle the indices\n",
    "test_images_shuffled = test_images[shuffled_indices]  # shuffle the rows of the dataset\n",
    "test_labels_class_shuffled = test_labels_class[shuffled_indices]\n",
    "test_labels_poison_shuffled = test_labels_poison[shuffled_indices]\n",
    "test_labels_species_shuffled = test_labels_species[shuffled_indices]\n",
    "test_variables_shuffled = test_variables[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some data augmentation!\n",
    "# Some horizontal flips? Random crops?\n",
    "\n",
    "def data_preprocessing(X, labels_class, labels_poison, labels_species, labels_variables, data_partition='train'):\n",
    "    '''Apply transformations and augmentations to training, validation, and test data;'''\n",
    "\n",
    "    CONTRAST_FACTOR = 3\n",
    "    DELTA = 0.3\n",
    "    \n",
    "    # image augmentation on training data\n",
    "    if data_partition==\"train\":\n",
    "        # adjust brightness\n",
    "        X_augm = tf.image.adjust_brightness(X, delta=DELTA) # FILL IN CODE HERE #\n",
    "\n",
    "        # adjust contrast\n",
    "        X_augm = tf.image.adjust_contrast(X_augm, contrast_factor=CONTRAST_FACTOR) # FILL IN CODE HERE #\n",
    "\n",
    "        # random flip\n",
    "        X_augm = tf.image.flip_left_right(X_augm) # FILL IN CODE HERE #\n",
    "\n",
    "        # concatenate original X and augmented X_aug data\n",
    "        X = tf.concat([X, X_augm],axis=0) # FILL IN CODE HERE #\n",
    "\n",
    "        # concatenate y_train (note the label is preserved)\n",
    "        labels_class_augm = labels_class\n",
    "        labels_class = tf.concat([labels_class, labels_class_augm],axis=0)\n",
    "\n",
    "        labels_poison_augm = labels_poison\n",
    "        labels_poison = tf.concat([labels_poison, labels_poison_augm],axis=0)\n",
    "\n",
    "        labels_species_augm = labels_species\n",
    "        labels_species = tf.concat([labels_species, labels_species_augm],axis=0)\n",
    "\n",
    "        labels_variables_augm = labels_variables\n",
    "        labels_variables = tf.concat([labels_variables, labels_variables_augm],axis=0)\n",
    "\n",
    "        # shuffle X and y, i.e., shuffle two tensors in the same order\n",
    "        shuffle = tf.random.shuffle(tf.range(tf.shape(X)[0], dtype=tf.int32))\n",
    "        X = tf.gather(X, shuffle).numpy() # transform X back to numpy array instead of tensor\n",
    "        labels_class = tf.gather(labels_class, shuffle).numpy() # transform y back to numpy array instead of tensor\n",
    "        labels_poison = tf.gather(labels_poison, shuffle).numpy()\n",
    "        labels_species = tf.gather(labels_species, shuffle).numpy()\n",
    "        labels_variables = tf.gather(labels_variables, shuffle).numpy()\n",
    "        \n",
    "        \n",
    "    # rescale image by dividing each pixel by 255.0 \n",
    "    # FILL IN CODE HERE #\n",
    "    X = X / 255.0\n",
    "    \n",
    "    return X, labels_class, labels_poison, labels_species, labels_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train images  (15638, 224, 224, 3)\n",
      "Shape of train labels  (15638,)\n",
      "Shape of train labels  (15638, 8)\n",
      "Shape of val images  (2285, 224, 224, 3)\n",
      "Shape of test images  (1911, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# apply data preprocessing\n",
    "train_images_shuffled, train_labels_class_shuffled, train_labels_poison_shuffled, train_labels_species_shuffled, train_variables_shuffled = data_preprocessing(train_images_shuffled, train_labels_class_shuffled, train_labels_poison_shuffled, train_labels_species_shuffled, train_variables_shuffled, data_partition='train')\n",
    "val_images_shuffled, val_labels_class_shuffled, val_labels_poison_shuffled, val_labels_species_shuffled, val_variables_shuffled = data_preprocessing(val_images_shuffled, val_labels_class_shuffled, val_labels_poison_shuffled, val_labels_species_shuffled, val_variables_shuffled, data_partition='val')\n",
    "test_images_shuffled, test_labels_class_shuffled, test_labels_poison_shuffled, test_labels_species_shuffled, test_variables_shuffled = data_preprocessing(test_images_shuffled, test_labels_class_shuffled, test_labels_poison_shuffled, test_labels_species_shuffled, test_variables_shuffled, data_partition='test')\n",
    "\n",
    "# print shapes\n",
    "print('Shape of train images ', train_images_shuffled.shape)\n",
    "print('Shape of train labels ', train_labels_class_shuffled.shape)\n",
    "print('Shape of train labels ', train_variables_shuffled.shape)\n",
    "print('Shape of val images ', val_images_shuffled.shape)\n",
    "print('Shape of test images ', test_images_shuffled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.035212210469942426"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.Series(train_labels_class).value_counts()  # 3156\n",
    "# pd.Series(train_labels_class).sum()  # 89628\n",
    "3156 / 89628  # 0.0352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(33,1)\n",
    "# # fig.subplots_adjust(hspace = 0.5, wspace= 0.5)\n",
    "\n",
    "# for i, label in enumerate(np.unique(train_labels_class)):  # iterate through the unique labels\n",
    "#     ax = axs[i]\n",
    "#     image = array_to_img(train_images[train_labels_class == label][0])  # get the first image of the current label\n",
    "#     ax.imshow(image)  # plot the image\n",
    "#     ax.set_title(label)\n",
    "#     ax.axis('off')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed and clear back end\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "# Convolutional Layer\n",
    "conv_layer = tf.keras.layers.Conv2D(32, kernel_size=4, padding=\"same\", activation=\"relu\")\n",
    "\n",
    "# Pooling Layer\n",
    "pooling_layer = tf.keras.layers.MaxPool2D()\n",
    "\n",
    "# Dropout Layer\n",
    "dropout_layer = tf.keras.layers.Dropout(0.25)\n",
    "\n",
    "# Flattening\n",
    "flat_layer = tf.keras.layers.Flatten()\n",
    "\n",
    "# Dense (Multiclassification Layer)\n",
    "num_classes = len(set(train_labels_class_shuffled))\n",
    "softmax_layer = tf.keras.layers.Dense(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    conv_layer,\n",
    "    pooling_layer,\n",
    "    dropout_layer,\n",
    "    flat_layer,\n",
    "    softmax_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15638, 224, 224, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_shuffled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(input_shape=(None, 224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,568</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">401408</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)             │    <span style=\"color: #00af00; text-decoration-color: #00af00\">13,246,497</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m1,568\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m401408\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m)             │    \u001b[38;5;34m13,246,497\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,248,065</span> (50.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,248,065\u001b[0m (50.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,248,065</span> (50.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,248,065\u001b[0m (50.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='accuracy',\n",
    "    verbose=1,\n",
    "    patience=5,\n",
    "    mode='max',\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m489/489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 207ms/step - accuracy: 0.0350 - loss: 3.8928 - val_accuracy: 0.0000e+00 - val_loss: 3.4965\n",
      "Epoch 2/10\n",
      "\u001b[1m489/489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 205ms/step - accuracy: 8.8479e-04 - loss: 3.4965 - val_accuracy: 0.0000e+00 - val_loss: 3.4965\n",
      "Epoch 3/10\n",
      "\u001b[1m489/489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 215ms/step - accuracy: 8.8479e-04 - loss: 3.4965 - val_accuracy: 0.0000e+00 - val_loss: 3.4965\n",
      "Epoch 4/10\n",
      "\u001b[1m489/489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 225ms/step - accuracy: 8.8479e-04 - loss: 3.4965 - val_accuracy: 0.0000e+00 - val_loss: 3.4965\n",
      "Epoch 5/10\n",
      "\u001b[1m489/489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 227ms/step - accuracy: 8.8479e-04 - loss: 3.4965 - val_accuracy: 0.0000e+00 - val_loss: 3.4965\n",
      "Epoch 6/10\n",
      "\u001b[1m489/489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 235ms/step - accuracy: 8.8479e-04 - loss: 3.4965 - val_accuracy: 0.0000e+00 - val_loss: 3.4965\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images_shuffled, train_labels_class_shuffled, epochs=10, validation_data=(val_images_shuffled, val_labels_class_shuffled), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.0000e+00 - loss: 3.4965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.4965062141418457, 0.0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images_shuffled, test_labels_class_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
