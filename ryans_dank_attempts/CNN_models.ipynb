{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df35d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "# tf and keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from PIL import ImageFile\n",
    "\n",
    "# sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78df06cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(metadata_path='fungi-clef-2025/metadata/FungiTastic-FewShot/', image_path='fungi-clef-2025/images/FungiTastic-FewShot/'):\n",
    "    \"\"\"Load the metadata for each data split.\"\"\"\n",
    "    # Load the metadata for each split\n",
    "    train_metadata = pd.read_csv(os.path.join(metadata_path, 'FungiTastic-FewShot-Train.csv'))\n",
    "    test_metadata = pd.read_csv(os.path.join(metadata_path, 'FungiTastic-FewShot-Val.csv'))\n",
    "\n",
    "    train_metadata = train_metadata.dropna(subset=[\"class\"])\n",
    "    train_metadata = train_metadata.groupby('class').filter(lambda x: len(x) > 1)\n",
    "\n",
    "    train_metadata, val_metadata = train_test_split(train_metadata, test_size=0.2, stratify=train_metadata[\"class\"])\n",
    "    \n",
    "    # Label each split\n",
    "    train_metadata[\"split\"] = \"train\"\n",
    "    val_metadata[\"split\"] = \"val\"\n",
    "    test_metadata[\"split\"] = \"test\"\n",
    "\n",
    "    # Join all of the data together\n",
    "    df_metadata = pd.concat([train_metadata, val_metadata, test_metadata])\n",
    "\n",
    "    # Add the full image location for each image\n",
    "    # Options for image size include 300p, 500p, 720p, fullsize \n",
    "    for idx, row in df_metadata.iterrows():\n",
    "        if row[\"split\"] in [\"train\", \"val\"]:\n",
    "            path = os.path.join(image_path, f\"train/300p/{row['filename']}\")\n",
    "        else:\n",
    "            path = os.path.join(image_path, f\"val/300p/{row['filename']}\")\n",
    "        df_metadata.at[idx, \"image_path\"] = path\n",
    "\n",
    "    return df_metadata\n",
    "\n",
    "\n",
    "def mapping(df, label):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df[label])\n",
    "    df[label + \"_label\"] = df[label]\n",
    "    df[label + \"_idx\"] = le.transform(df[label])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_random_augmentation(X):\n",
    "    random_number = random.randint(1, 4)\n",
    "\n",
    "    if random_number == 1:\n",
    "        return tf.image.flip_left_right(X)\n",
    "    if random_number == 2:\n",
    "        return tf.image.flip_up_down(X)\n",
    "    if random_number == 3:\n",
    "        return tf.image.adjust_brightness(X, delta=0.3)\n",
    "    if random_number == 4:\n",
    "        return tf.image.adjust_contrast(X, contrast_factor=3)\n",
    "\n",
    "\n",
    "def preprocess_image_tf(image, target_size):\n",
    "    # Load image using designated filepath\n",
    "    img = load_img(image)\n",
    "\n",
    "    # Get original dimensions\n",
    "    original_height = tf.cast(tf.shape(img)[0], tf.float32)\n",
    "    original_width = tf.cast(tf.shape(img)[1], tf.float32)\n",
    "    \n",
    "    # Calculate scaling factor to maintain aspect ratio\n",
    "    height_scale = target_size / original_height\n",
    "    width_scale = target_size / original_width\n",
    "    scale = tf.minimum(height_scale, width_scale)\n",
    "    \n",
    "    # Calculate new dimensions\n",
    "    new_height = tf.cast(tf.math.round(original_height * scale), tf.int32)\n",
    "    new_width = tf.cast(tf.math.round(original_width * scale), tf.int32)\n",
    "    \n",
    "    # Resize the image while maintaining aspect ratio\n",
    "    resized_img = tf.image.resize(img, [new_height, new_width], method='bilinear')\n",
    "    \n",
    "    # Use resize_with_pad to add padding to make the image square\n",
    "    padded_img = tf.image.resize_with_pad(\n",
    "        resized_img, \n",
    "        target_size, \n",
    "        target_size, \n",
    "        method='bilinear'\n",
    "    )\n",
    "    \n",
    "    # Normalize pixel values to [0,1]\n",
    "    normalized_img = tf.cast(padded_img, tf.float32) / 255.0\n",
    "    \n",
    "    return normalized_img\n",
    "\n",
    "\n",
    "def load_images_and_labels(df, image_size):\n",
    "    \"\"\"Load the images and labels based on the metadata frame passed in.\"\"\"\n",
    "    images = []\n",
    "    labels_class = []\n",
    "    # labels_poison = []\n",
    "    # labels_species = []\n",
    "    variables = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Load and save the image as an array\n",
    "        # img = load_img(row[\"image_path\"], target_size=image_size)\n",
    "        img = preprocess_image_tf(row[\"image_path\"], image_size)\n",
    "        img_arr = img_to_array(img)\n",
    "        images.append(img_arr)\n",
    "\n",
    "        # Append the class to the list of labels\n",
    "        labels_class.append(row[\"class_idx\"])\n",
    "\n",
    "        # labels_poison.append(row[\"poisonous\"])\n",
    "        # labels_species.append(row[\"species_idx\"])\n",
    "        # variables.append((row[\"latitude\"], row[\"longitude\"], row[\"elevation\"], row[\"countryCode\"], row[\"region\"], row[\"substrate\"], row[\"habitat\"], row[\"landcover\"]))\n",
    "        variables.append((row[\"elevation\"], row[\"habitat\"]))\n",
    "\n",
    "    # Stack and convert into a numpy array\n",
    "    images = np.stack(images)\n",
    "\n",
    "    # Cast label list to np.array for easier manipulation\n",
    "    labels_class = np.array(labels_class)\n",
    "    # labels_poison = np.array(labels_poison)\n",
    "    # labels_species = np.array(labels_species)\n",
    "    variables = np.array(variables)\n",
    "\n",
    "    return images, labels_class, variables #, labels_poison, labels_species, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b9f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322b569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "028897ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_df = load_metadata()\n",
    "\n",
    "md_df = mapping(md_df, \"class\")\n",
    "\n",
    "images, labels_class, variables = load_images_and_labels(md_df, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d69af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-split the images and their labels\n",
    "train_idx = md_df[\"split\"] == \"train\"\n",
    "val_idx = md_df[\"split\"] == \"val\"\n",
    "test_idx = md_df[\"split\"] == \"test\"\n",
    "\n",
    "train_images = images[train_idx]\n",
    "train_labels_class = labels_class[train_idx]\n",
    "# train_labels_poison = labels_poison[train_idx]\n",
    "# train_labels_species = labels_species[train_idx]\n",
    "train_variables = variables[train_idx]\n",
    "\n",
    "val_images = images[val_idx]\n",
    "val_labels_class = labels_class[val_idx]\n",
    "# val_labels_poison = labels_poison[val_idx]\n",
    "# val_labels_species = labels_species[val_idx]\n",
    "val_variables = variables[val_idx]\n",
    "\n",
    "test_images = images[test_idx]\n",
    "test_labels_class = labels_class[test_idx]\n",
    "# test_labels_poison = labels_poison[test_idx]\n",
    "# test_labels_species = labels_species[test_idx]\n",
    "test_variables = variables[test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "825b7e36",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "label_counts = {label: np.sum(train_labels_class == label) for label in np.unique(train_labels_class)}\n",
    "\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "augmented_variables = []\n",
    "\n",
    "for class_label, label_count in label_counts.items():\n",
    "    # The number of images that need to be augmented and appended to reach 1000\n",
    "    images_to_augment = 1000 - label_count\n",
    "    \n",
    "    # Pool of potential images to augment\n",
    "    image_pool_idx = [i for i, label in enumerate(train_labels_class) if label == class_label]\n",
    "\n",
    "    for i in range(images_to_augment):\n",
    "\n",
    "        # Select a random image to augment\n",
    "        image_idx = random.choice(image_pool_idx)\n",
    "        image_to_aug = images[image_idx]\n",
    "        variables_to_aug = variables[image_idx]  # get the associated metadata\n",
    "\n",
    "        # Apply a random augmentation\n",
    "        augmented = apply_random_augmentation(image_to_aug)\n",
    "\n",
    "        # Save new image and label\n",
    "        augmented_images.append(augmented)\n",
    "        augmented_labels.append(class_label)\n",
    "        augmented_variables.append(variables_to_aug)\n",
    "\n",
    "# Rescale all of the images so they're pixel value is between [0, 1]\n",
    "augmented_images = [augmented_image / 255.0 for augmented_image in augmented_images]\n",
    "\n",
    "# Add to the existing\n",
    "train_images = np.concatenate((train_images, np.array(augmented_images)), axis=0)\n",
    "train_labels_class = np.concatenate((train_labels_class, np.array(augmented_labels)), axis=0)\n",
    "train_variables = np.concatenate((train_variables, np.array(augmented_variables)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac6e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts.\n",
    "train_images.\n",
    "train_labels_class.\n",
    "train_variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1a9995",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape train images: {train_images.shape}\")\n",
    "print(f\"Shape train classes: {train_labels_class.shape}\")\n",
    "# print(f\"Shape train poison: {train_labels_poison.shape}\")\n",
    "# print(f\"Shape train species: {train_labels_species.shape}\")\n",
    "print(f\"Shape train variables: {train_variables.shape}\")\n",
    "print(f\"Shape val images: {val_images.shape}\")\n",
    "print(f\"Shape test images: {test_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the training images\n",
    "indices = list(range(train_images.shape[0]))  # create a list of indices of the size of the dataset\n",
    "\n",
    "shuffled_indices = np.random.permutation(indices)  # shuffle the indices\n",
    "\n",
    "train_images_shuffled = train_images[shuffled_indices]  # shuffle the rows of the dataset\n",
    "train_labels_class_shuffled = train_labels_class[shuffled_indices]\n",
    "# train_labels_poison_shuffled = train_labels_poison[shuffled_indices]\n",
    "# train_labels_species_shuffled = train_labels_species[shuffled_indices]\n",
    "train_variables_shuffled = train_variables[shuffled_indices]\n",
    "\n",
    "\n",
    "# Shuffle the validation images\n",
    "indices = list(range(val_images.shape[0]))  # create a list of indices of the size of the dataset\n",
    "shuffled_indices = np.random.permutation(indices)  # shuffle the indices\n",
    "val_images_shuffled = val_images[shuffled_indices]  # shuffle the rows of the dataset\n",
    "val_labels_class_shuffled = val_labels_class[shuffled_indices]\n",
    "# val_labels_poison_shuffled = val_labels_poison[shuffled_indices]\n",
    "# val_labels_species_shuffled = val_labels_species[shuffled_indices]\n",
    "val_variables_shuffled = val_variables[shuffled_indices]\n",
    "\n",
    "\n",
    "# Shuffle the test images\n",
    "indices = list(range(test_images.shape[0]))  # create a list of indices of the size of the dataset\n",
    "shuffled_indices = np.random.permutation(indices)  # shuffle the indices\n",
    "test_images_shuffled = test_images[shuffled_indices]  # shuffle the rows of the dataset\n",
    "test_labels_class_shuffled = test_labels_class[shuffled_indices]\n",
    "# test_labels_poison_shuffled = test_labels_poison[shuffled_indices]\n",
    "# test_labels_species_shuffled = test_labels_species[shuffled_indices]\n",
    "test_variables_shuffled = test_variables[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cfb804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some data augmentation!\n",
    "# Some horizontal flips? Random crops?\n",
    "\n",
    "def data_preprocessing(X, labels_class, labels_variables, data_partition='train'):\n",
    "    '''Apply transformations and augmentations to training, validation, and test data;'''\n",
    "\n",
    "    CONTRAST_FACTOR = 3\n",
    "    DELTA = 0.3\n",
    "    \n",
    "    # image augmentation on training data\n",
    "    if data_partition==\"train\":\n",
    "        # adjust brightness\n",
    "        X_augm = tf.image.adjust_brightness(X, delta=DELTA) # FILL IN CODE HERE #\n",
    "\n",
    "        # adjust contrast\n",
    "        X_augm = tf.image.adjust_contrast(X_augm, contrast_factor=CONTRAST_FACTOR) # FILL IN CODE HERE #\n",
    "\n",
    "        # random flip\n",
    "        X_augm = tf.image.flip_left_right(X_augm) # FILL IN CODE HERE #\n",
    "\n",
    "        # concatenate original X and augmented X_aug data\n",
    "        X = tf.concat([X, X_augm],axis=0) # FILL IN CODE HERE #\n",
    "\n",
    "        # concatenate y_train (note the label is preserved)\n",
    "        labels_class_augm = labels_class\n",
    "        labels_class = tf.concat([labels_class, labels_class_augm],axis=0)\n",
    "\n",
    "        # labels_poison_augm = labels_poison\n",
    "        # labels_poison = tf.concat([labels_poison, labels_poison_augm],axis=0)\n",
    "\n",
    "        # labels_species_augm = labels_species\n",
    "        # labels_species = tf.concat([labels_species, labels_species_augm],axis=0)\n",
    "\n",
    "        labels_variables_augm = labels_variables\n",
    "        labels_variables = tf.concat([labels_variables, labels_variables_augm],axis=0)\n",
    "\n",
    "        # shuffle X and y, i.e., shuffle two tensors in the same order\n",
    "        shuffle = tf.random.shuffle(tf.range(tf.shape(X)[0], dtype=tf.int32))\n",
    "        X = tf.gather(X, shuffle).numpy() # transform X back to numpy array instead of tensor\n",
    "        labels_class = tf.gather(labels_class, shuffle).numpy() # transform y back to numpy array instead of tensor\n",
    "        # labels_poison = tf.gather(labels_poison, shuffle).numpy()\n",
    "        # labels_species = tf.gather(labels_species, shuffle).numpy()\n",
    "        labels_variables = tf.gather(labels_variables, shuffle).numpy()\n",
    "        \n",
    "        \n",
    "    # rescale image by dividing each pixel by 255.0 \n",
    "    # FILL IN CODE HERE #\n",
    "    X = X / 255.0\n",
    "    \n",
    "    return X, labels_class, labels_variables #, labels_poison, labels_species, labels_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply data preprocessing\n",
    "train_images_shuffled, train_labels_class_shuffled, train_variables_shuffled = data_preprocessing(train_images_shuffled, train_labels_class_shuffled, train_variables_shuffled, data_partition='train')\n",
    "val_images_shuffled, val_labels_class_shuffled, val_variables_shuffled = data_preprocessing(val_images_shuffled, val_labels_class_shuffled, val_variables_shuffled, data_partition='val')\n",
    "test_images_shuffled, test_labels_class_shuffled, test_variables_shuffled = data_preprocessing(test_images_shuffled, test_labels_class_shuffled, test_variables_shuffled, data_partition='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print shapes\n",
    "print('Shape of train images ', train_images_shuffled.shape)\n",
    "print('Shape of train labels ', train_labels_class_shuffled.shape)\n",
    "print('Shape of val images ', val_images_shuffled.shape)\n",
    "print('Shape of val labels ', val_labels_class_shuffled.shape)\n",
    "print('Shape of test images ', test_images_shuffled.shape)\n",
    "print('Shape of test labels ', test_labels_class_shuffled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57342f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(train_labels_class_shuffled))\n",
    "print(np.unique(val_labels_class_shuffled))\n",
    "print(np.unique(test_labels_class_shuffled))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f6ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out invalid labels from the test data\n",
    "valid_indices = test_labels_class_shuffled < 30  # Keep only labels less than 30\n",
    "\n",
    "# Apply the filter to both images and labels\n",
    "test_images_shuffled = test_images_shuffled[valid_indices]\n",
    "test_labels_class_shuffled = test_labels_class_shuffled[valid_indices]\n",
    "test_variables_shuffled = test_variables_shuffled[valid_indices]\n",
    "\n",
    "# Verify the result\n",
    "print(f\"Filtered test images shape: {test_images_shuffled.shape}\")\n",
    "print(f\"Filtered test labels shape: {test_labels_class_shuffled.shape}\")\n",
    "print(f\"Unique test labels: {np.unique(test_labels_class_shuffled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3769930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed and clear back end\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "# Convolutional Layer\n",
    "conv_layer = tf.keras.layers.Conv2D(32, kernel_size=4, padding=\"same\", activation=\"relu\")\n",
    "\n",
    "# Pooling Layer\n",
    "pooling_layer = tf.keras.layers.MaxPool2D()\n",
    "\n",
    "# Dropout Layer\n",
    "dropout_layer = tf.keras.layers.Dropout(0.25)\n",
    "\n",
    "# Flattening\n",
    "flat_layer = tf.keras.layers.Flatten()\n",
    "\n",
    "# Dense (Multiclassification Layer)\n",
    "num_classes = len(set(train_labels_class_shuffled))\n",
    "softmax_layer = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "\n",
    "model_1 = tf.keras.Sequential([\n",
    "    conv_layer,\n",
    "    pooling_layer,\n",
    "    dropout_layer,\n",
    "    flat_layer,\n",
    "    softmax_layer\n",
    "])\n",
    "\n",
    "# build and compile model\n",
    "model_1.build(input_shape=(None, 224, 224, 3))\n",
    "\n",
    "model_1.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# print model_tf summary\n",
    "### YOUR CODE HERE ###\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5318aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='accuracy',\n",
    "    verbose=1,\n",
    "    patience=5,\n",
    "    mode='max',\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71337b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_shuffled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d601b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9224266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model_tf on (X_train, y_train) data\n",
    "### YOUR CODE HERE ###\n",
    "history = model_1.fit(train_images_shuffled, train_labels_class_shuffled, epochs=10, validation_data=(val_images_shuffled, val_labels_class_shuffled), callbacks=[early_stopping])\n",
    "print('Total params: ', model_1.count_params())\n",
    "\n",
    "# plot loss curves\n",
    "### YOUR CODE HERE ###\n",
    "trained_model_epochs = history.history\n",
    "x_arr = np.arange(len(trained_model_epochs['loss'])) + 1 \n",
    "plt.plot(x_arr, trained_model_epochs['loss'], '-o', label='Train loss')\n",
    "plt.plot(x_arr, trained_model_epochs['val_loss'], '--<', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# evaluate the accuracy of model_tf on (X_train, y_train) and (X_val, y_val)\n",
    "### YOUR CODE HERE ###\n",
    "print('Training Accuracy:', trained_model_epochs['accuracy'][-1])\n",
    "print('Validation Accuracy:', trained_model_epochs['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd6e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Accuracy:', model_1.evaluate(test_images_shuffled, test_labels_class_shuffled)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab739779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "model_2 = tf.keras.Sequential()\n",
    "\n",
    "# add convolutional layer\n",
    "### YOUR CODE HERE ###\n",
    "model_2.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(4,4), strides=(1,1), padding='same', name='conv_1', activation='relu'))\n",
    "model_2.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(4,4), strides=(1,1), padding='same', name='conv_2', activation='relu'))\n",
    "\n",
    "# add max pooling layer \n",
    "### YOUR CODE HERE ###\n",
    "model_2.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# add dropout layer\n",
    "### YOUR CODE HERE ###\n",
    "model_2.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "\n",
    "# add a flattening layer\n",
    "### YOUR CODE HERE ###\n",
    "model_2.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# add the classification layer\n",
    "### YOUR CODE HERE ###\n",
    "num_classes = len(set(train_labels_class_shuffled))\n",
    "model_2.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# build and compile model\n",
    "model_2.build(input_shape=(None, 224, 224, 3))\n",
    "model_2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "# print model_tf summary\n",
    "### YOUR CODE HERE ###\n",
    "print(model_2.summary())\n",
    "\n",
    "# train model_tf on (X_train, y_train) data\n",
    "### YOUR CODE HERE ###\n",
    "trained_model = model_2.fit(train_images_shuffled, train_labels_class_shuffled, epochs=10, validation_data=(val_images_shuffled, val_labels_class_shuffled), callbacks=[early_stopping])\n",
    "print('Total params: ', model_2.count_params())\n",
    "\n",
    "# plot loss curves\n",
    "### YOUR CODE HERE ###\n",
    "trained_model_epochs = trained_model.history\n",
    "x_arr = np.arange(len(trained_model_epochs['loss'])) + 1 \n",
    "plt.plot(x_arr, trained_model_epochs['loss'], '-o', label='Train loss')\n",
    "plt.plot(x_arr, trained_model_epochs['val_loss'], '--<', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# evaluate the accuracy of model_tf on (X_train, y_train) and (X_val, y_val)\n",
    "### YOUR CODE HERE ###\n",
    "print('Training Accuracy:', trained_model_epochs['accuracy'][-1])\n",
    "print('Validation Accuracy:', trained_model_epochs['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc42f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Accuracy:', model_2.evaluate(test_images_shuffled, test_labels_class_shuffled)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682595c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
